{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords extraction with [KeyBERT](https://maartengr.github.io/KeyBERT/)\n",
    "This notebook demonstrates keyword extraction using KeyBERT. The article titles per day and per website are consolidated, and keywords are extracted from this text. To process the German texts, the [distiluse-base-multilingual-cased-v1](https://www.sbert.net/docs/pretrained_models.html) Sentence Transformers model was selected.\n",
    "\n",
    "Since the model has a maximum sequence length of 128, the idea was to divide the text into shorter sequences and extract the keywords for each sequence. It was determined that for each day/website, 5 sequences would be created, from which 2 keywords would be extracted. This approach should cover the majority of the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christoph\\.conda\\envs\\DIGCRE23\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model = SentenceTransformer(\"distiluse-base-multilingual-cased-v1\")\n",
    "kw_model = KeyBERT(model=sentence_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles = os.listdir('./data/articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['23_11_18_23_blick.csv',\n",
       " '23_11_18_23_srf.csv',\n",
       " '23_11_19_23_blick.csv',\n",
       " '23_11_19_23_srf.csv',\n",
       " '23_11_20_23_blick.csv',\n",
       " '23_11_20_23_srf.csv',\n",
       " '23_11_21_23_blick.csv',\n",
       " '23_11_21_23_srf.csv',\n",
       " '23_11_22_23_blick.csv',\n",
       " '23_11_22_23_srf.csv',\n",
       " '23_11_23_23_blick.csv',\n",
       " '23_11_23_23_srf.csv',\n",
       " '23_11_24_23_blick.csv',\n",
       " '23_11_24_23_srf.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame for all keywords\n",
    "df_keywords = pd.DataFrame(columns=['file', 'section', 'keyword'])\n",
    "# Number of sections to split the content into\n",
    "num_sections = 5\n",
    "\n",
    "for df_name in df_articles:\n",
    "    df_file = pd.read_csv(f'./data/articles/{df_name}')\n",
    "    \n",
    "    # Convert all titles to list\n",
    "    all_content = df_file['title'].tolist()\n",
    "    # all_content to one string\n",
    "    all_content = ' '.join(all_content)\n",
    "    \n",
    "    # Split the content into sections\n",
    "    section_length = math.ceil(len(all_content) / num_sections)\n",
    "    content_sections = [all_content[i:i+section_length] for i in range(0, len(all_content), section_length)]\n",
    "\n",
    "    # Number of sections in the article\n",
    "    num_sections_actual = len(content_sections)\n",
    "    \n",
    "    # Create a temporary DataFrame\n",
    "    df_temp = pd.DataFrame(columns=['file', 'section', 'keyword'])\n",
    "    \n",
    "    # Extract keywords for each section\n",
    "    for i, section in enumerate(content_sections):\n",
    "        keywords = kw_model.extract_keywords(section, keyphrase_ngram_range=(1, 1), stop_words=None, top_n=2)\n",
    "        \n",
    "        # Extract only the words from the keywords\n",
    "        keywords_without_prob = [word for word, _ in keywords]\n",
    "        \n",
    "        # Add the keywords to the temporary DataFrame\n",
    "        if keywords_without_prob:\n",
    "            df_temp = pd.concat([df_temp, pd.DataFrame({'file': [df_name], 'section': [i+1], 'keyword': [keywords_without_prob]})], ignore_index=True)\n",
    "    \n",
    "    # Add the temporary DataFrame to the main DataFrame\n",
    "    df_keywords = pd.concat([df_keywords, df_temp], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the keywords for each section\n",
    "combined_df = df_keywords.groupby('file').agg({'section': list, 'keyword': sum}).reset_index()\n",
    "combined_df.columns = ['file', 'sections_combined', 'keywords_combined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [schweiz, gefahr, unfall, feuer, ukrainischen,...\n",
       "1     [politik, wofür, thailand, energieversorger, w...\n",
       "2     [blumen, mobilisierung, crash, golfbälle, wolf...\n",
       "3     [wahlsiegerin, kandidat, linke, linkem, schwei...\n",
       "4     [tunnelwand, flugzeugunglück, neuenkirch, kant...\n",
       "5     [reichsten, wahrheit, einwanderung, neue, weib...\n",
       "6     [schweizer, schnee, verkehrsunfall, minivan, n...\n",
       "7     [recycling, rapperswil, bundeshaus, orban, pos...\n",
       "8     [tragische, tragischer, schweizern, ukraine, i...\n",
       "9     [rechtspopulisten, kinderspital, elefantenkuh,...\n",
       "10    [flughafenpolizei, polizei, schweiz, sexualdel...\n",
       "11    [schweizerinnen, schweizer, schweiz, rabbiner,...\n",
       "12    [sterben, spanier, ostschweizer, reparaturarbe...\n",
       "13    [atommacht, schweizer, medizinstudium, studier...\n",
       "Name: keywords_combined, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df['keywords_combined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.drop(columns=['sections_combined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>keywords_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23_11_18_23_blick.csv</td>\n",
       "      <td>[schweiz, gefahr, unfall, feuer, ukrainischen,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23_11_18_23_srf.csv</td>\n",
       "      <td>[politik, wofür, thailand, energieversorger, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23_11_19_23_blick.csv</td>\n",
       "      <td>[blumen, mobilisierung, crash, golfbälle, wolf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23_11_19_23_srf.csv</td>\n",
       "      <td>[wahlsiegerin, kandidat, linke, linkem, schwei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23_11_20_23_blick.csv</td>\n",
       "      <td>[tunnelwand, flugzeugunglück, neuenkirch, kant...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    file                                  keywords_combined\n",
       "0  23_11_18_23_blick.csv  [schweiz, gefahr, unfall, feuer, ukrainischen,...\n",
       "1    23_11_18_23_srf.csv  [politik, wofür, thailand, energieversorger, w...\n",
       "2  23_11_19_23_blick.csv  [blumen, mobilisierung, crash, golfbälle, wolf...\n",
       "3    23_11_19_23_srf.csv  [wahlsiegerin, kandidat, linke, linkem, schwei...\n",
       "4  23_11_20_23_blick.csv  [tunnelwand, flugzeugunglück, neuenkirch, kant..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store dataframe as csv file\n",
    "combined_df.to_csv('./data/articles_keywords_23_11_18-23.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DIGCRE23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
